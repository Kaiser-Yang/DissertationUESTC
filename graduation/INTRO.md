# Communication-Efficient Learning of Deep Networks from Decentralized Data

Features of FL:

- Non-IID: 各个节点所拥有的数据仅仅代表个人用户的数据分布，而不是整体分布
- Unbalanced: 各个节点的数据量可能会有很大的差异
- Massively Distributed: 参与训练的节点数量非常多
- Limited communication: 各个节点的通信能力有限

Two ways to add computation:

1. increased parallelism: 使用更多的工作节点参与计算
2. increased computation: 各个工作节点在每次通信之间进行更多的本地计算

Federated SGD: 各个节点在每次通信之间进行一次本地SGD更新
Federated Averaging: 各个节点在每次通信之间进行多个本地SGD更新，然后将模型参数进行平均

Conclusions:

- 通过实难说明了增加计算量可以在一定程度上减少收敛所需要的通信轮数。

# Large Scale Distributed Deep Networks

DistBelief framework

同时使用两种并行方式来提升模型的训练速度：

1. 将训练数据划分到多个机器上进行数据并行
2. 在每个节点上也进行并行计算

提出了参数服务器分片的概念，在一次分布式机器学习中可以存在多个参数服务器，每个参数服务器负责存储和更新模型的一部分参数。

Downpour SGD: 在使用多个参数服务器的基础上使用异步的方式来对整个模型进行训练。具体地，每个工作节点会周期性地从参数服务器拉取最新的模型参数，然后在本地使用SGD对模型进行更新，最后将更新后的参数发送回参数服务器。由于各个工作节点是异步进行计算的，因此参数服务器可能会接收到过时的梯度信息。
Sandblaster L-BFGS: 在计算过程中先分配较少的数据给各个工作节点进行训练，在完成后再分配更多的数据进行训练。这样可以减少等待较慢工作节点的时间，从而提升整体的训练速度。

# Parallelized stochastic gradient descent

首次提出了并行化随机梯度下降（SGD）的方法，旨在加速大规模机器学习模型的训练过程。

# Revisiting distributed synchronous SGD

## 异步随机梯度下降（Async-SGD）

机制：多个worker并行处理数据，各自：

* 从参数服务器获取当前参数
* 计算梯度
* 将梯度发送回参数服务器更新

---

问题：梯度“陈旧性”

每个worker计算梯度时使用的参数版本可能已经过时

worker越多，梯度平均陈旧步数越多（N个worker时平均陈旧N-1步）

可能导致训练不稳定，噪声增加


## 重新审视同步随机梯度下降（Sync-SGD）

机制：参数服务器等待所有worker完成计算

* 聚合所有梯度
* 一次性更新参数
* 保证算法是真正的“小批量SGD”

---

优势：消除了参数版本不一致问题

缺点：

- 批次大小显著增加
- 受最慢worker速度限制（木桶效应）

---

改进方案: 备份worker策略

当收到足够数量的worker梯度后立即更新，丢弃慢worker的梯度

混合方法尝试：组内同步+组间异步（实验效果不佳）

# Staleness-aware async-sgd for distributed deep learning

本文的核心创新是提出了一种过期感知的异步SGD变体算法。

* 核心思想：跟踪每次梯度计算对应的过期步数，并据此动态调整学习率。
* 具体方法：非常简单有效——将基础学习率除以该梯度的过期值。过期越严重（步数落后越多），该梯度更新对参数的影响就越小。

# Asynchronous distributed ADMM for consensus optimization

提出异步ADMM算法，其在训练过程中主节点不会等待所有的工作节点完成更新，而是允许部分工作节点的更新被接受，从而提高了分布式优化的效率。该算法还保证计算较慢的节点的更新不会被忽略，从而确保了算法的公平性和收敛性。

问题模型：考虑将一个全局优化目标分解为多个节点上的局部目标之和，并将其建模为全局变量共识优化问题，即要求所有节点上的局部解最终达成一致。

---

传统（同步）ADMM流程：

* 工作节点：并行优化各自的局部目标。
* 主节点：等待所有工作节点的结果，然后更新“共识变量”以驱动各节点解达成一致，再将新共识广播回去。

此过程必须严格同步，存在上述瓶颈。

---

本文提出的异步ADMM：

* 部分同步：主节点无需等待所有工作节点，只需达到一定数量的更新即可推进。
* 延迟有界：允许较快的节点更新更频繁，但强制规定来自较慢节点的更新不能“过期”太久（设置一个最大延迟界限）。

# Ako: Decentralised deep learning with partial gradient exchange

可扩展的去中心化同步：部分梯度交换

- 做法：每个工作节点定期计算梯度更新，但不发送整个梯度。它将梯度按可用网络带宽分割成多个部分，在每一轮同步中，只与其他节点交换一部分。
- 优势：每次通信量小，网络带宽被充分利用。虽然每个节点每次只收到梯度的一部分，但经过多轮交换，所有节点最终都能在有限延迟内获得完整的梯度信息，从而保证收敛。

---

解耦的CPU与网络利用

- 做法：每个工作节点内部将计算任务与网络通信任务解耦。
- 计算任务：全力、持续地训练模型，生成梯度更新。
- 网络任务：异步地收集和累积梯度，并在发送前进行分区，确保在保持恒定延迟的同时，完全饱和网络带宽。
- 优势：计算和通信并行不悖，互不阻塞，实现了硬件资源的最大化利用。

# Dlion: Decentralized distributed deep learning in micro-clouds

DLion，一个去中心化的分布式深度学习系统，旨在缩短训练时间、提高模型精度并处理系统可扩展性。其核心技术包括：

* 计算能力感知的批处理：根据节点的实时计算能力动态调整批处理大小。
* 网络感知的数据交换：根据网络带宽和延迟状况，智能调整梯度或模型参数的交换策略。
* 选择性数据传播：并非所有数据都需要全局同步，可能根据重要性或相关性进行选择性传播。

# Distributed training of deep learning models: A taxonomic perspective

我们旨在通过分析深度学习模型训练的通用特性，以及如何在集群中分配此类工作负载以实现协同模型训练，揭示在独立机器集群中训练深度神经网络时起作用的基本原理。为此，我们概述了当代分布式深度学习系统采用的各类技术，并探讨其对训练过程的影响与意义。为系统化理解和比较这些技术，我们将不同方法归类整理，从而构建了分布式深度学习系统的分类体系。

# Federated learning with non-iid data

两大难题：

* 通信挑战：已有较多研究，旨在减少深度网络权重矩阵的传输成本，应对设备掉线、同步延迟等问题。主要技术包括：
    * 安全聚合协议：保护隐私的同时进行高效聚合。
    * 结构化/草图化更新、深度梯度压缩：将通信带宽降低两个数量级。

* 统计挑战：
    * 问题本质：联邦学习依赖SGD，而SGD的理论保证基于训练数据的独立同分布假设。在实际中，每个边缘设备上的本地数据天然是非IID的（例如，不同用户的手机图片主题完全不同）。
    * 严重性：作者通过实验证明，在高度偏斜的非IID数据下，使用经典联邦平均算法训练模型的准确率会急剧下降（CIFAR-10下降51%，关键词识别下降55%）。

理论分析：

* 将准确率下降归因于 “权重发散”——即相同初始化下，不同训练过程产生的权重差异。
* 从理论上证明了权重发散的上界与每个客户端的数据分布和全局数据分布之间的推土机距离有关，并受学习率、同步步数等影响。这为问题提供了数学解释。

实践方案：

* 提出了一种数据共享策略，旨在用极少量（如5%）的全局共享数据来校准和改善非IID数据下的训练。
* 这引入了一个关键的权衡：在模型准确率与数据集中化程度之间进行折衷。实验表明，仅共享5%的数据就能将CIFAR-10的准确率提升30%。

# Local SGD Converges Fast and Communicates Little

文章的核心是给出了该算法的收敛速率上界，并得出了关键结论：

收敛保证
完美线性计算加速
可以用远低于传统同步SGD的通信开销，获得相同的计算加速收益。

两种提升“计算/通信比”的策略：

* 增大批量大小 (b) - 传统策略。
* 增大通信间隔 (H) - 本文倡导的策略。

两者关系：在b和H较小时，两种策略都能带来近似的线性加速效果。

重要限制：与批量大小不能无限增大一样，通信间隔H也不能无限增大，否则会影响收敛。论文在第4节给出了实际选择指南。

激发非凸研究：虽然分析针对凸问题，但这一积极结果为在更重要的非凸场景（如深度学习）中研究本地SGD提供了动力和信心。

# Federated optimization in heterogeneous networks

论文开篇明确指出，虽然FedAvg是联邦学习的事实标准，但其在处理现实异质性时存在根本缺陷：

* 系统异质性：FedAvg要求所有参与设备完成固定轮数（E轮）的本地训练。对于能力较弱的设备，常见做法是直接丢弃其更新，这浪费了数据和计算资源。
* 统计异质性：当设备数据呈非独立同分布时，FedAvg已被证明在实验中会发散或表现不佳。更重要的是，FedAvg在此类现实场景下缺乏收敛性理论保证，其行为难以被刻画和分析。

基于对上述问题的深刻理解，论文提出了FedProx，其核心思想是：

* 关键洞察：系统异质性与统计异质性相互交织。简单丢弃掉队设备，或无条件合并其不完整的更新，都会隐式地加剧统计异质性，损害收敛。
* 核心机制：在本地目标函数中引入一个近端项。
    * 作用：该近端项将本地更新锚定在上一次全局模型附近，从而稳定训练过程。
    * 优势：它为服务器提供了一种原则性的方法，来“信任”并融合来自不同设备、可能因系统限制而计算程度不同的局部更新，从而显式地建模和缓解异质性。

主要贡献

* 提出了FedProx框架，允许设备根据自身系统能力执行可变量的本地工作（而不仅仅是固定E轮），并通过近端项稳健地聚合这些异构的更新。
* 首次为联邦优化算法在同时考虑系统和统计异质性的真实场景下，提供了收敛性保证。这填补了FedAvg的理论空白，为算法行为提供了严格的分析工具。
* 在合成与真实数据集上的实验表明，FedProx显著提升了在高度异构网络中的稳定性和最终精度，平均绝对测试精度提升了22%。

# FedDM: Federated Learning Incorporating Dissimilarity Measure for Mobile Edge Computing Systems

论文聚焦于在资源受限、数据异构的MEC环境中应用联邦学习。尽管FedProx通过引入近端项有效应对了异质性挑战，但它仍存在两个局限，未能充分利用异构信息并优化资源消耗：

* 近端项系数是固定值。
* 本地训练轮数是固定值。

解决方案：FedDM 的两大动态机制

* 自适应近端项系数：
    * 方法：不再使用固定系数，而是根据参数差异性和损失差异性这两种度量，来动态更新近端项的权重。
    * 作用：让本地模型的更新能够更智能地权衡“向全局模型靠拢”与“拟合本地数据”之间的关系，从而提升收敛效率与模型精度。
* 动态本地训练轮数：
    * 方法：在整个联邦学习过程中，根据资源状况和模型状态，动态调整每个设备本地训练的轮数。
    * 作用：在MEC系统中，边缘设备的计算能力差异很大。此机制允许能力强的设备多算，能力弱的设备少算，避免“一刀切”造成的资源浪费或掉队，实现更高效的资源利用。

主要贡献

* 提出了FedDM算法，通过联合优化自适应近端项系数和动态训练轮数，在资源受限条件下提升联邦学习性能。
* 为所提出的方法提供了收敛性理论分析。
* 在多个真实数据集、不同异构数据设置、不同网络模型和系统配置下进行了广泛实验，验证了FedDM在有限通信资源下的有效性与收敛性。

# Fault-Tolerant Decentralized Distributed Asynchronous Federated Learning with Adaptive Termination Detection

联邦学习正从依赖中心服务器的集中式架构，转向客户端点对点通信的去中心化架构。后者能提升可扩展性和鲁棒性（避免单点故障）。
即使在可控的局域网环境中，消息延迟、设备性能差异等也使得“同步执行”的假设不切实际。这些因素会导致客户端进展不一致和模型发散。

* 异步性：没有全局时钟，客户端以各自步调运行。
* 容错性：必须处理客户端崩溃、掉线等良性故障。
* 终止检测：在去中心化异步系统中，如何判断整个分布式训练任务已经完成（收敛）并安全停止是一个经典难题。缺乏明确的终止条件会导致过早停止（未收敛）或资源浪费（过度训练）。

解决方案：两大机制

* 客户端自信收敛机制：
    * 做法：每个客户端自主监测本地训练过程的稳定性（例如，模型参数或损失的变化）。
    * 目标：当客户端自行判断已达到足够的局部收敛时，才触发终止流程。这无需中心服务器协调。
* 客户端响应式终止协议：
    * 做法：当一个客户端发出终止信号后，接收该信号的客户端会更新自己的终止标志，并将该信号随其后续的模型广播继续传播出去。
    * 目标：使终止信号像“涟漪”一样在网络中可靠扩散，最终实现所有客户端的协同、一致关闭，避免无限期训练。

主要贡献

* 提出完全去中心化的收敛机制：客户端自主监控，无中心协调。
* 设计客户端响应式终止协议：解决分布式终止检测难题，确保训练在收敛后可靠停止。
* 在实际多机环境中进行验证：在更接近真实复杂性的多机联邦学习环境中实验，证明方法的有效性。

# Mobility-aware cluster federated learning in hierarchical wireless networks

移动感知集群联邦学习（MACFL）算法

文章系统性地指出，在包含移动用户的大规模层次化无线网络中，现有联邦学习方案存在严重不足：

* 理论分析缺失：鲜有工作量化分析用户移动性对联邦学习收敛速度和精度的影响。
* 分簇策略僵化：传统静态分簇方案既不高效（增加通信计算开销），也不实用（忽略了用户因移动而导致的物理可接入性变化）。如何为移动用户动态分簇是难题。
* 聚合方案瓶颈：传统等权重平均聚合方式，在非独立同分布数据和用户移动性导致的数据分布动态变化下，已成为性能瓶颈。

针对上述挑战，文章提出了移动感知聚类联邦学习算法，其主要创新点包括：

* 建立含移动性的理论分析框架：
    * 首次在分析层次化联邦学习的收敛速率时，同时纳入用户移动性、数据异构性和网络架构异构性的影响，为算法设计奠定了理论基础。
* 设计移动用户的高效参与机制：
    * 重新设计接入机制：允许移动用户有效参与协作，即使他们在本地训练期间可能离开当前接入点的覆盖范围。
    * 引入个性化联邦学习：为应对数据异构，采用个性化技术，使模型能更好地适应每个用户。
    * 引入注意力加权平均：改进传统聚合方案，根据客户端数据的重要性或相似性进行加权聚合，以提升模型精度和收敛速度。

# AFedAvg: Communication-efficient federated learning aggregation with adaptive communication frequency and gradient sparse

这篇文章的核心目标是减少联邦学习（FL）中的通信开销，同时尽量保持模型性能。作者在经典 FedAvg 框架上做了两方面的改进：一是自适应通信频率（adaptive communication frequency），即不是所有客户端每轮都上传更新，而是根据某些准则动态决定上传频次；二是梯度/模型更新的稀疏化（gradient sparsification），即只上传重要的梯度坐标或压缩更新量。两者结合旨在在通信受限或异构网络下显著降低上传字节数，同时通过合理聚合和误差补偿保持收敛性与精度。

(1) 分析通信频率对联邦学习收敛性的影响。通过实验发现，在模型训练的不同阶段调整通信频率，可实现更快的收敛速度和近乎最高精度；
(2) 提出新型聚合算法AFedAvg，通过通信压缩降低单次通信成本，其独特的参数量自适应引导频率选择机制进一步降低总通信成本。
(3) 在典型图像数据集的多数据分布实验中验证：本方法融合梯度稀疏性与通信延迟的策略，显著优于基线算法FedAvg及其变体方案。

论文开篇指出现有联邦学习系统在向更实用的三层架构演进时，面临一个核心的“两难”控制问题：

* 通信瓶颈的转移：传统的客户端-云架构存在远程通信瓶颈。客户端-边缘-云架构通过在边缘进行本地聚合，减少了上传至云端的流量，缓解了此问题。
* 新的控制难题：在这种三层架构中，聚合频率（客户端与边缘、边缘与云之间的同步频率）成为一个关键但难以设定的“旋钮”。
    * 聚合太频繁：通信开销大，拖慢整体训练速度。
    * 聚合太少：可能导致模型过拟合、质量下降，且在不同步时造成资源浪费（快的节点等慢的节点）。
* 环境复杂性：客户端和边缘节点的资源异构性与动态性，使得手动或固定频率的设置方式完全失效。

FedAda：

* 理论奠基：首先量化了聚合频率与模型收敛性之间的数学关系，为整个方法提供了坚实的收敛性理论分析基础。
* 算法设计：
    * 基于理论分析，提出近似算法计算保证模型质量的基准聚合频率。
    * 进一步设计在线自适应调整策略，根据节点的实时计算与通信能力，动态调整基准频率以最大化训练效率。
* 联合优化：创新性地同时优化边缘层的本地聚合频率和云层的全局聚合频率，从而在资源受限的动态异构网络中，全局平衡计算负载与两级通信开销。

# AdaptiveFL: Communication-Adaptive Federated Learning Under Dynamic Bandwidth

在训练过程中会对当前的带宽进行估计，同时根据估计的结果选取最适应的子模型与服务器进行通信，从而提升联邦学习在动态带宽环境下的通信效率。
核心问题：动态带宽——被忽视的现实瓶颈

论文开篇指出了一个被现有研究所忽视的关键现实问题：

* 通信成本高昂：联邦学习中频繁的模型参数交换是主要瓶颈，尤其是在模型和客户端数量激增的今天。
* 现有方案的静态假设：为了提升通信效率，主流方法（如LotteryFL、FedSpa等）普遍采用模型稀疏化（剪枝） 技术来减少传输量。然而，这些方法都隐含地假设网络带宽在每一轮训练中是稳定不变的。
* 现实的动态性：地理分布的异构设备（手机、汽车、无人机等）实际处于动态带宽环境中，网络条件时刻波动。实验证明，在这种动态环境下，现有方法的模型性能会下降（如准确率降低4.27%），收敛时间更长。其根本原因在于信道的不一致性——固定大小的压缩模型无法适应实时变化的带宽。

解决方案：

* 高性能本地训练方法：
    * 目标：训练一个 “可裁剪的”本地模型。
    * 技术：采用随机Frank-Wolfe方法进行本地训练。这使得训练出的模型能够被剪枝至任意稀疏度，而无需重新训练，且剪枝后的子模型仍能保持有竞争力的精度。
* 自适应通信策略：
    * 机制：在每个通信轮次中，根据客户端当前实时的可用带宽，动态选择最合适的子模型（即特定稀疏度的模型） 上传至服务器。
    * 优势：如图1所示，与现有固定剪枝比例的方案相比，AdaptiveFL能实现带宽的实时、最佳利用——带宽宽时传大一些的子模型（信息更丰富），带宽窄时传小一些的子模型（确保及时更新），从而最大化通信效率和模型更新质量。

# Communication-Efficient Federated Learning with Adaptive Number of Participants

论文精准地指出了一个在联邦学习中被广泛研究（选择哪些客户端）但深度不足的关键维度：每轮应该选择多少个客户端？

* 固有挑战：联邦学习面临数据异构、通信效率等核心瓶颈。常见的应对策略包括增加本地计算、利用并行性以及部分参与（避免等待过慢的客户端）。
* 关键权衡：参与客户端的数量直接影响训练效率，但存在一个根本矛盾：
    * 参与过多：能利用更丰富的梯度多样性，加速收敛，但会导致单轮通信成本激增，可能压垮有限资源或造成参与者掉线。
    * 参与过少：虽降低了单轮开销，但会减慢学习速度，损害模型泛化能力。
* 研究空白：现有主流方法（如FedAvg、FedProx）通常固定每轮的参与客户端数量，未能将这一关键参数动态化以优化全局效率。

解决方案：ISP（智能参与者选择）

核心思想：动态决策每轮的参与客户端数量。
目标：在当前轮次中，选择足以产生有效训练影响的最合适的客户端数量，从而在通信开销和收敛速度之间达成最优平衡。

# 1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs

早期挑战：在深度学习兴起初期，训练大型模型（尤其是语音识别用的全连接深度神经网络）非常耗时。虽然图像处理的稀疏网络能成功并行化，但全连接的DNN并行化效率不高。
数据并行的通信瓶颈：在数据并行中，每个节点计算子梯度，但汇总和分发这些与模型等大的梯度需要极高的节点间通信带宽，这直接限制了扩展性。

论文提出了一个激进但有效的解决方案：

* 1比特梯度量化：将每个梯度值仅用1个比特（即二值化） 来表示，从而将通信数据量压缩至极致。
* 误差反馈技术：这是该方法成功的关键。单纯量化会引入误差，损害收敛。作者提出，必须将当前小批次的量化误差累积起来，加到下一个小批次的梯度上，再进行下一次量化。
    * 作用：确保长期来看，所有梯度信息的“期望值”是正确的，量化误差不会在训练中累积并导致偏离。这类似于信号处理中的Σ-Δ调制。
    * 意义：这一“误差补偿”思想，成为后来所有梯度压缩技术的核心原理之一。

# Scalable distributed DNN training using commodity GPU cloud computing

论文提出了一个深刻的观点：许多加速SGD的技术（如小批量、动量法）都可以视为权重更新的延迟。通信延迟（如双缓冲、异步SGD）也是一种延迟形式。这为主动设计延迟策略奠定了理论基础。

关键技术：稀疏化、残差累积与量化

基于“梯度稀疏”的观察，论文设计了一套完整的技术方案：

* 阈值稀疏化：
    * 做法：只传输绝对值超过阈值 τ 的梯度元素，其他“微小”更新被延迟。
    * 编码：使用“键值对”映射（索引 + 梯度值）来编码稀疏梯度，大幅减少传输量。
* 梯度残差与误差反馈：
    * 关键机制：未被传输的小梯度值并非被丢弃，而是累加在一个本地 “梯度残差” 变量中。
    * 作用：确保具有微小但持续偏向的梯度最终能被更新（只是频率更低），从而维持了算法的长期收敛性。这本质上是为不同量级的梯度元素实现了可变长度的智能延迟。
* 极致量化：
    * 做法：将梯度值量化为 +τ 或 -τ（1比特），并将量化误差加回梯度残差（误差反馈）。
    * 惊喜发现：1比特量化足以维持精度和收敛速度，这与之前“1比特量化”论文的结论一致。结合稀疏化，最终每个更新仅需传输权重索引和一个符号位。
* 进一步的熵编码：
    * 指出可通过Golomb-Rice编码对稀疏索引的“间隔”进行压缩，理论上可将每个权重更新的平均成本降至10-11比特，实现额外3倍的压缩。
* “航位推测”式同步
    * 机制：采用无锁、异步的“航位推测” 方式。节点间只传输权重的增量（+τ或-τ），接收方按相同逻辑本地应用这些增量。
    * 要求：只要更新逻辑满足交换律，增量可以以任何顺序接收，这天然适合异步网络环境。

# Communication-efficient federated learning with adaptive quantization

* 隐私需求：催生了联邦学习，但隐私保护技术（如差分隐私、同态加密）本身会增加通信开销。
* 效率瓶颈：为降低开销，出现了梯度量化技术（如QSGD、1-bit SGD），但现有方法多为固定量化位数，且在常见的非独立同分布数据下效率仍不足。
* 现实挑战：边缘设备不可靠，客户端可能随时掉线，这给训练带来了噪声和不确定性。

自适应量化梯度：

* 核心思想：打破固定位数量化。根据梯度更新的“信息量”动态分配量化资源。
* 具体机制：更新幅度大的梯度，用更多比特进行精细量化；更新幅度小的梯度，用更少比特进行粗糙量化甚至跳过。这实现了通信比特的“按需分配”，在整体上显著减少传输总量。

应对客户端掉线的增强机制：

* 问题：客户端随机掉线会为梯度估计引入噪声。
* 方案：结合方差缩减技术，对传输的梯度进行适当的放大校正，以保持梯度估计的无偏性，从而提升算法在掉线情况下的鲁棒性和收敛稳定性。

算法创新：提出AQG方法，实现了基于梯度重要性的自适应位数量化。
理论保证：提供了算法的收敛性理论分析。
鲁棒性验证：实验证明AQG能承受高达90%的客户端掉线率，其增强版本在常见的中等掉线规模下能实现更显著的通信减少。

# Communication-Efficient Federated Learning via Clipped Uniform Quantization

通过采用最优截断阈值和客户端自适应量化方案，该方法在保持竞争性准确率的同时，显著降低了客户端与服务器间模型权重传输所需的带宽和内存需求。方案通过基于均方量化误差倒数的权重平均机制，有效平衡了通信效率与模型精度的权衡关系。与联邦平均不同，该设计无需向服务器披露客户端特定数据量，从而增强了客户端隐私保护。

# Communication-efficient federated learning via quantized compressed sensing

论文首先系统梳理了解决联邦学习通信瓶颈的两类主流技术及其不足：

* 梯度量化：如1比特量化（仅传符号），但高量化误差可能影响收敛。
* 梯度稀疏化：丢弃不重要的梯度值，但单纯稀疏化压缩率有限。
* 量化压缩感知：结合两者，但现有方案要么需要额外传输抖动信号增加开销，要么仅限于1比特量化灵活性不足，且都未能系统解决如何在参数服务器端最小化重建误差这一核心问题。

解决方案：FedQCS框架的三步压缩与两种重建

FedQCS的核心是一个完整的“压缩-传输-重建”管道：

* 设备端三步压缩策略：
    * 块稀疏化：将梯度向量分块，每块内仅保留幅度最大的部分值。
    * 降维（随机投影）：对稀疏化后的块使用感知矩阵进行降维，进一步压缩。
    * 量化：对降维后的值使用最优的非均匀标量量化器进行多比特量化。
    * 灵活性：通过调整量化比特数和降维比，可灵活控制每个梯度条目所需的比特数，实现比1比特量化更高的压缩比。
* 参数服务器端两种重建策略：
    * 先估计后聚合：首先从压缩信号中估计出每个设备的局部梯度块（使用改进的EM-GAMP算法解决量化CS重建问题），然后再聚合。此策略最小化估计误差，但计算复杂度随设备数量线性增长。
    * 先聚合后估计（创新点）：先对压缩信号进行聚合，再估计聚合后的梯度。利用Bussgang定理将非线性量化问题转化为带噪声的线性CS问题，然后用EM-GAMP算法求解。此策略通过改变聚合的块数来权衡性能与复杂度，更适合大规模场景。

# Privacy preserving machine learning with homomorphic encryption and federated learning

论文指出了一个常被忽视的深层次隐私风险：虽然联邦学习不共享原始数据，但传输的明文梯度仍然可能泄露信息。

* 具体攻击：文中提到了 “成员推断攻击” ，即恶意参与者可以利用接收到的梯度更新，训练一个“影子模型”来推断其他参与者的私有数据特征。
* 本质矛盾：这意味着，标准的联邦学习协议在面临内部恶意节点或服务器被攻破时，其隐私保护是不完备的。

解决方案：基于同态加密的隐私保护框架

PFMLP 的核心是使用 Paillier同态加密算法来构建一个加密的联邦学习流程。

* 核心技术：同态加密允许在密文状态下直接进行数学运算（如加法、乘法），解密后的结果与对明文进行相同运算的结果一致。

工作流程：

* 客户端在本地计算梯度后，先使用同态加密算法对梯度进行加密。
* 将加密后的梯度发送给服务器。
* 服务器在不解密的情况下，直接对收到的密文梯度进行聚合操作（如加权平均）。
* 将聚合后的加密全局模型更新发回给客户端。
* 客户端解密后，获得更新后的模型。

安全保证：在整个训练过程中，服务器和任何第三方都无法看到明文的梯度或模型参数，从根本上防御了基于梯度分析的隐私攻击。

# Communication-Efficient and Privacy-Adaptable Mechanism for Federated Learning

论文指出，当前联邦学习中对通信效率（压缩）和隐私保护的处理通常是分离且次优的：

* “先差分隐私，再量化”的常规流程：先添加用于隐私保护的噪声（如高斯噪声），再对含噪模型进行量化压缩。
* 双重弊端：
    * 精度损失叠加：隐私噪声和量化噪声都会损害模型更新精度，两者叠加导致最终模型性能下降。
    * 资源浪费：量化噪声没有贡献任何额外的隐私保护效用，仅仅是通信开销的产物，是一种“无效噪声”。

CEPAM 的核心思想是：设计一种机制，使得为压缩通信而产生的量化噪声，其统计特性恰好满足差分隐私的要求。它基于最新的 RSUQ（随机缩放均匀量化） 技术构建。

* 核心技术原理（RSUQ）：这是一种随机向量量化技术。它的神奇之处在于，能将量化过程引入的失真（误差）转换为一个与待量化数据独立、且方差可控的加性噪声项。
* CEPAM的工作方式：利用RSUQ的这一特性，精心设计量化过程，使得产生的量化噪声的分布完全符合中心化差分隐私（DP）所要求的高斯或拉普拉斯分布。
    * CEPAM-Gaussian：产生精确的高斯噪声。
    * CEPAM-Laplace：产生精确的拉普拉斯噪声。
* 一举两得的优势：
    * 通信压缩：作为向量量化器，RSUQ本身就实现了高效的模型更新压缩。
    * 隐私保护：量化过程中“天然”产生的、为压缩服务的噪声，直接被“塑造”为满足差分隐私要求的噪声，无需额外添加。压缩噪声即隐私噪声。

# Privacy-preserving deep learning

Distributed selective stochastic gradient descent (DSSGD):

每个节点在本地计算梯度时，只选择一部分梯度进行更新，从而减少了需要传输的数据量，并且保护了数据的隐私。

论文精准地指出了深度学习时代的两大核心矛盾，这些问题后来成为驱动联邦学习发展的根本动力：

* 隐私风险：集中收集用户数据（照片、语音）存在永久存储、意外敏感信息泄露、法律传唤等严重隐私问题。
* 数据垄断与法规壁垒：互联网巨头垄断数据导致垄断AI模型；而在医疗等领域，法规禁止数据共享，导致各机构只能在小规模、同质化数据上训练，模型效果差、易过拟合。

核心解决方案：选择性参数共享

作者提出的方案是一个无需共享原始数据的协同学习系统，其核心创新在于：

* 共享对象：不共享数据，而是共享训练过程中神经网络的部分参数。
* 运行机制：在随机梯度下降过程中，穿插进行本地参数更新和选择性参数共享。参与者从他人那里获得少量参数值来更新自己的模型。
* 核心优势：
    * 有效性：即使只共享一小部分参数（如1%或10%），也能帮助各参与者的模型跳出局部最优，获得接近集中式训练（拥有全部数据）的准确率。
    * 实用性：SGD算法本身对异步、不可靠的更新具有鲁棒性，使得该框架能自然容忍参与者掉线、更新延迟等问题。
    * 隐私性：泄露的仅是一小部分间接的模型参数，而非原始数据，隐私风险大幅降低。

实验验证与隐私增强

效果验证：在MNIST和SVHN数据集上，仅共享1%参数就能将准确率从个体训练的 93.16% 提升至 98.71% （MNIST），接近集中式训练的 99.17% 。
隐私增强：文章进一步指出，可以通过差分隐私技术（稀疏向量技术）对共享的参数值进行加噪保护，从而在“选择性共享”的基础上，实现可量化的隐私-效用权衡。

# Practical secure aggregation for privacy-preserving machine learning

论文开篇点明在移动设备上利用敏感数据进行机器学习的巨大潜力与风险，并引出了核心解决方案——安全聚合。

* 隐私需求：用户设备在参与联邦学习时，不希望其明文模型更新被服务提供商或其他用户看到。
* 安全聚合的目标：计算多方更新的总和，但任何一方（包括聚合服务器）都无法看到单个用户的明文更新。
* 严苛的工程约束：在移动设备场景下，协议设计必须满足：
    * 通信高效：开销不应超过明文传输更新数据的两倍。
    * 完全鲁棒：必须能够完全容忍用户在任何时刻掉线，而不影响其他用户的隐私或聚合的正确性。

设计一个专为移动联邦学习优化的安全聚合协议。

协议的核心设计目标与特性

* 恒定轮次：通信往返次数固定，有利于降低延迟。
* 低通信开销：旨在最小化数据传输量。
* 容错性：能够处理参与方失败或掉线的情况。
* 简化的信任模型：仅需一个有限信任的服务器（服务器负责路由消息和计算最终结果，但不能窥探单个用户的输入）。

论文提供了两种方案，以权衡效率与安全性：

针对半诚实敌手的变体：

* 安全模型：假设所有参与者（包括服务器）会遵循协议流程，但会好奇地试图从接收到的消息中推断他人的私密输入（“诚实但好奇”）。
* 优势：更高效，且在标准模型下可证明安全。

针对主动敌手的变体：

* 安全模型：可抵御包括服务器在内的主动恶意攻击者（可能偏离协议、篡改消息）。
* 代价：需要额外一轮通信，安全性证明依赖于随机预言机模型。
* 安全性：两种变体都通过基于模拟的证明（MPC协议的标准方法）严格保证了服务器只能学习到聚合后的结果，而无法获知任何单个用户的输入。

# CoCoA: A General Framework for Communication-Efficient Distributed Optimization

论文首先犀利地指出了当时（约2017-2018年）最主流的分布式优化范式——小批量方法的内在缺陷：

* 僵化的权衡：小批量方法通过增加批量大小来减少通信轮次，但理论收敛速度会随着批量增大而下降，最终退化为传统的批处理方法。这导致它无法灵活适配不同系统和问题对通信与计算的最优权衡需求。
* 方法论的局限：小批量方法通常是将单机求解器直接并行化，其算法和分析往往针对特定问题定制，缺乏普适性，在适用范围之外可能失效。

CoCoA 框架旨在克服上述局限，其核心思想是利用对偶理论和数据划分，将原问题优雅地分解为可在各机器上独立求解的子问题。

通用框架与对偶分解：

* 核心：该框架适用于一大类凸优化问题。它根据数据是按特征划分还是按样本划分，并利用原问题-对偶问题的数学结构，将全局目标自然分解为多个子问题。
* 优势：这种基于对偶的分解，不仅使得方法高效，还能带来原-对偶收敛保证，并允许计算对偶间隙作为精度证明和停止准则。

两大关键灵活性：

* 灵活的本地求解器：每个机器可以使用任意现成的、先进的单机求解器（如针对特定模型优化的库），实现了最佳单机性能的即插即用。
* 灵活的通信方案：通信量可以根据具体问题和系统情况轻松定制，允许在分布式环境中大幅减少通信。算法没有必须调优的超参数，默认参数即可保证收敛。

理论保证：

* 提供了严格的收敛速率
* 关键优势：其收敛速度不随机器数量 K 的增加而降低，并且允许子问题以任意精度求解，这为实现高度灵活的计算-通信权衡提供了理论根基。

只适用的凸优化。

# Adding vs. Averaging in Distributed Primal-Dual Optimization

论文首先肯定了CoCoA框架通过增加本地计算来减少通信的思路，但指出了其一个关键缺陷：

* “更新稀释”问题：随着机器数量 K 的增加，每台机器的更新在全局聚合时会被平均稀释（效果除以K）。即使其他机器已接近其子问题的最优解，它们也不得不等待，导致收敛速度随K增大而显著下降。

COCOA+ 的核心创新在于重新设计分配给每个工作节点的本地子问题：
* 核心机制：修改后的子问题使得各机器计算出的本地更新，可以在全局聚合时进行“积极的相加”，而不仅仅是取平均。
* 效果：这避免了更新被稀释，从而在理论和实验上都获得了快得多的收敛速度。其收敛速率在最坏情况下也与机器数量K无关，这是其实现强扩展性的关键。
* 强扩展性的定义：在总数据量固定的情况下，增加机器数量，算法仍能保持高效的收敛速度。这与许多分布式方法（增加机器反而增加通信负担）形成鲜明对比。

理论扩展：

* 将收敛性分析从光滑损失函数扩展到非光滑损失函数（如支持向量机、非光滑回归），扩大了适用范围。
* 为CoCoA和COCOA+都提供了更强的原-对偶收敛率分析，这能用于构建高效的优化质量证明和停止准则。

实验验证：

* 在真实分布式数据集上的实验证实，随着机器数K增加，COCOA+ 表现出强扩展性，而包括原始CoCoA在内的其他方法速度则显著下降。
* 保持灵活性：与CoCoA一样，COCOA+ 仍允许每台机器使用任意本地求解器，保持了框架的实用性。

只适用的凸优化。

# Trading computation for communication: Distributed stochastic dual coordinate ascent

问题：经典的正则化损失最小化问题在监督学习中普遍存在。虽然已有许多高效的单机算法，但面对超大规模数据（数十亿样本），需要利用成百上千的CPU核心进行分布式求解。

研究空白与动机：主流分布式优化方法基于（随机）梯度下降或ADMM。然而，近年来在单机环境中，（随机）对偶坐标上升（DCA/SDCA）算法被证明具备可比甚至优于梯度下降方法的收敛速度。但其在分布式环境下的研究匮乏，缺乏与主流方法的系统对比。

本文目标：弥合这一差距，开发分布式随机对偶坐标上升算法，并将其与基于SGD和ADMM的分布式算法进行对比。

DisDCA算法的核心设计融合了对偶坐标上升的思想与分布式计算的需求：

* 并行计算：在 K 台机器（或核心）上进行并行计算。
* 本地顺序更新：每台机器在每次迭代中顺序更新 m 个对偶变量。
* 全局通信聚合：随后进行一个 “规约” 步骤，在所有进程间通信以同步更新。
* 理论保证：该算法对光滑和非光滑损失函数均能提供较强的收敛速率保证。

计算与通信的权衡分析：核心分析了两个关键参数 m（每轮更新的对偶变量数）和 K（机器数）带来的权衡。

* 直觉：增加 m 旨在减少收敛所需迭代次数，从而缓解通信压力。
* 理论：揭示了 m、K 与正则化参数 λ 构成的有效区域关系，为参数调优提供了理论指导。

实践对比：提出了DisDCA的一个实用变体，并与分布式ADMM进行了比较。通过在相同分布式框架下与基于SGD和ADMM的算法进行实验，验证了理论分析并证明了DisDCA的有效性

只适用于凸优化。

# Communication-efficient distributed optimization using an approximate newton-type method

提出了一种基于近似牛顿法的分布式优化方法，旨在减少分布式优化中的通信开销。该方法通过在每个节点上计算局部的二阶信息来近似全局的二阶信息，从而减少了需要传输的数据量。

线性收敛率：对于二次目标函数，该方法的收敛率是线性的，并且随着数据规模的增加而显著提高。

迭代次数恒定：在合理的假设下，该方法所需的迭代次数基本上是常数，这意味着即使数据规模增加，计算成本也不会显著增加。

适用于随机优化：该方法特别适用于随机优化和学习问题，能够有效处理大规模数据集。

# Communication-efficient distributed optimization of self-concordant empirical loss

只适用于凸优化。该算法基于非精确阻尼牛顿法，其中非精确牛顿步长由分布式预条件共轭梯度法计算。在监督学习的标准设置中，当问题条件数随样本量平方根增长时，该算法所需通信轮数不会随样本量增加，且仅随机器数量缓慢增长。

# Communication-efficient algorithms for statistical optimization

论文聚焦于超大规模数据下的经验风险最小化问题。当数据无法存储在单机内存时，必须采用分布式方法。核心挑战在于：如何在通信有限（同步少、通信量小）的前提下，设计能利用更多数据以获得更高统计精度的分布式估计算法。

论文深入研究了两种通信高效的算法：

AVGM（平均混合算法）：

* 做法：将总数为N的数据均匀分给m台机器（每台n=N/m个样本）。每台机器计算其本地数据的经验最小化解θ_i，然后简单平均所有θ_i。
* 优点：极其通信高效（仅需一轮通信），实现简单，对机器故障或速度差异鲁棒。
* 理论贡献（核心）：本文首次严格证明，在一组合理的总体风险条件下，AVGM的均方误差以 O(1/(nm) + 1/n²) 的速率衰减。这意味着，只要机器数量m小于每台机器的样本数n，AVGM就能达到与使用全部N个样本的集中式算法相匹配的最优收敛速率。这回答了“分布式平均是否真的比单机使用部分数据更好”这一基本问题。

SAVGM（子采样平均混合算法）：

* 做法：在AVGM基础上，每台机器进一步对自己的数据进行子采样，用子样本估计其本地估计器θ_i的偏差，并返回一个经过偏差校正的估计。
* 理论优势：其均方误差衰减为 O(1/(mn) + 1/n³)。当 m < n² 时，其一阶主项与集中式黄金标准匹配，且二阶项比AVGM更小，精度更高。

论文在模拟数据（正态/非正态回归模型）和一个超大规模的真实在线广告点击预测任务（约2.4亿样本，74万维度，55GB）上进行了详细实验，验证了AVGM和SAVGM的优异性能，甚至接近无法实现的“使用全部数据的黄金标准”，并展示了SAVGM相对于朴素方案和AVGM的显著优势。

# TSEngine: Enable efficient communication overlay in distributed machine learning in WANs

提出 TSEngine 将发送给中央服务器的数据分散到多个边缘节点，从而减少了中央服务器的通信负载，提高了分布式机器学习在广域网中的效率。但是需要全连通的拓扑结构。

论文首先精准地指出了在广域网环境下，标准参数服务器架构存在一个底层通信模式上的根本性缺陷：

* 星型通信模式：在模型分发（服务器到所有节点）和模型聚合（所有节点到服务器）时，服务器作为唯一的发送者或接收者，会同时与所有节点通信。
* 通信洪泛问题：这导致来自/发往所有节点的流量同时冲击服务器的网络端口，造成严重的网络拥塞，极大地延长了通信时间，成为训练效率的主要瓶颈。
* 现有方案的不足：已有的优化方案（如利用可编程交换机或构建静态的树状覆盖网络）要么难以在广域网中部署，要么无法适应广域网固有的动态与异构特性（网络状况波动、资源竞争）。

TSEngine 的核心思想是摒弃静态、固定的通信逻辑，转而采用实时感知网络、动态调度传输的智能覆盖网络。

两大自适应调度协议：

* 针对模型分发：自学习通信调度协议：
    * 机制：不再由服务器一次性广播给所有节点。而是动态地将已收到模型的节点指定为新的发送者，让其去分发给其他节点（类似于P2P扩散）。
    * 智能性：通过已完成传输的反馈信息自动学习网络状态，并在线优化调度策略。
* 针对模型聚合：最小等待延迟通信调度协议：
    * 机制：动态调度那些已完成本地计算的“就绪”节点，在到达服务器之前，先在节点之间进行部分模型的聚合，形成分层聚合。
    * 目标：优先减少因网络异构性（快慢节点）造成的同步等待延迟。

系统优势：

* 非侵入性：作为独立的通信层实现，无需修改训练算法、操作系统或网络硬件，易于部署。
* 高效性：实验表明，在128个节点的广域网集群中，相比星型模式，能将模型分发和聚合的通信时间分别降低95%和90%，整体训练时间显著缩短。

# Sancus: staleness-aware communication-avoiding full-graph decentralized training in large-scale graph neural networks

论文精准地指出了图神经网络在分布式训练中面临的两大核心挑战，使其有别于传统的MLP或CNN：

* 计算与通信的耦合：GNN的迭代邻域聚合过程，需要在训练中不断查询目标节点及其多跳邻居，导致嵌入向量和梯度都需要在设备间大量传输。通信成本可占总训练时间的80%以上。
* 现有系统架构的不足：
    * 集中式参数服务器：存在单点瓶颈、带宽限制和预处理复杂等问题。
    * 现有去中心化方案：如CAGNET，虽然消除了中心点，但要求所有嵌入和梯度的冗余广播，并且仍需同步等待所有设备，通信开销依然巨大。

Sancus 从计算、通信和系统设计三个层面进行了重构：

* 计算重构：将GNN视为纯矩阵运算序列
    * 关键视角转变：不再将GNN计算视为基于图语义的邻域访问，而是将其纯粹看作一连串的矩阵乘法（邻接矩阵 × 嵌入矩阵）。
    * 数据分布：将庞大的邻接矩阵和嵌入矩阵切片后，分发到各个GPU上。每个GPU仅处理自己负责的子矩阵块，计算本地聚合结果。
* 通信避免：引入有界过时的历史嵌入
    * 核心机制：为了避免在每轮迭代中都进行昂贵的跨GPU嵌入传输，Sancus允许GPU在计算时使用缓存的历史嵌入值，而非总是获取最新的嵌入。
    * “有界过时”：系统引入一套新颖的度量标准，来管理和控制这些历史嵌入的最大过时程度，确保它们不会太“旧”而损害模型收敛。
* 收敛性理论保证：
    * 论文证明了在使用有界过时历史嵌入的情况下，嵌入和梯度的近似误差是有界的，并且系统整体具备收敛性保证。这为其实用性提供了理论基础。

主要贡献

问题新视角：提出通过矩阵化视角和有界过时历史嵌入来加速分布式GNN。
新度量标准：定义了去中心化GNN中有界嵌入过时的度量体系。
新理论准则：提供了Sancus的通信成本上界，并证明了其误差有界和收敛性。
卓越性能：在大型基准数据集上的实验表明，Sancus能显著提升效率（减少通信），同时保持模型精度几乎没有损失。

该论文是针对分布式图神经网络的，提出了SANCUS框架，通过在每个计算节点缓存历史嵌入向量，并设计了一种陈旧性感知的自适应广播跳过机制，显著降低了节点间的通信量。

# Topology-aware federated learning in edge computing: A comprehensive survey

- 本研究通过采用边缘网络拓扑结构（即网络架构）对独特联邦学习工作进行分类，开创性地提出了全新视角。
- 我们将联邦学习全面划分为四大拓扑结构：星型拓扑、网状拓扑、混合拓扑及较少见的网络拓扑，为未来研究提供了清晰的分类框架。
- 论文筛选过程遵循系统性综述方法，采用PRISMA[86]指南规范操作。
- 本文阐述设计方案、基线模型及基准测试体系，并深入剖析若干代表性研究的核心发现。
- 针对拓扑感知联邦学习的未来发展，勾勒出具有前景的研究方向与挑战课题。

# Dynamic Topology Optimization for Efficient and Decentralised Federated Learning

论文指出了一个常被理论研究忽视，但在实际应用中至关重要的现实问题：联邦学习部署在电池供电的设备（如物联网、移动医疗设备）上时，不仅面临通信效率的挑战，还面临能量消耗的硬约束。现有研究在同时应对这两种约束方面存在空白。

论文基于一个完全去中心化的共识联邦学习算法 FedL-Con，并提出了一个核心创新：不将网络拓扑视为固定或随机的，而是将其作为一个可通过优化问题主动设计的变量。

理论基础：借鉴共识理论中的拓扑设计概念。共识算法的收敛速度直接受网络拓扑（用拉普拉斯矩阵描述）影响。

* 核心方法：
    * 形式化优化问题：将寻找最优拓扑的问题，构建为一个优化问题。
    * 纳入现实约束：在问题中明确纳入设备能量限制和通信限制作为约束条件。
    * 求解最优拓扑：通过求解该优化问题，得到一个拉普拉斯矩阵，该矩阵描述的拓扑能在满足设备能量与通信预算的前提下，确保共识协议（即联邦学习）的收敛。
* 优势：
    * 主动适应：使系统能主动应对设备故障、电量变化导致的拓扑改变，增强鲁棒性。
    * 无缝集成：该拓扑优化算法可以无缝集成到任何基于拓扑的联邦学习范式中，作为底层的网络组织层。

主要贡献

* 提出了一种新颖的、可集成到联邦学习中的能量感知拓扑优化算法。
* 形式化了一个考虑设备能量与通信限制的拓扑设计优化问题。
* 在一个移动医疗场景中验证了该方法的有效性。

这类工作关注的是“去中心化联邦学习（decentralised FL / decentralized SGD）”中如何动态调整节点间的通信拓扑，使学习既能快速收敛又能节省通信资源并提升鲁棒性。核心思想是：不使用单一固定网络结构，而是依据节点状态（如梯度相似度、信道质量、负载与能量）和目标（最小通信量、最大收敛速率、隐私/鲁棒性）周期性或在线地优化/重构邻接关系与链路权重，从而在资源受限与网络动态环境下取得更好性能。

# Two-Timescale Energy Optimization for Wireless Federated Learning

使用李雅普诺夫优化框架将原问题拆解成并行的子问题，在大尺度上对稳定的参数进行更新，在小尺度上对快速变化的参数进行更新，从而实现无线联邦学习中的能量优化。

# Energy-efficient dynamic asynchronous federated learning in mobile edge computing networks

论文精准地指出了在移动边缘计算网络中部署联邦学习所面临的两个核心现实挑战：

* 环境动态性：移动设备存在不确定性（离线、断电），且数据量分布不均、随时间变化。
* 资源与能耗约束：为提升效率和稳定性，必须考虑资源限制和动态网络环境，而现有研究对训练过程中的能耗关注相对较少。
* 现有方法的不足：
    * 同步方法（如FedAvg）：等待所有设备，在动态MEC网络中效率低下。
    * 完全异步方法（如FedAsync）：服务器收到一个更新就聚合，虽灵活但可能导致模型收敛不稳定、需要更多训练轮次，反而增加总能耗。

本文提出了一套从模型到调度算法的完整解决方案：

* DAFL模型设计：
    * 基于FedAsync（完全异步），但克服了其收敛困难。具体机制未在此段详述，但目标是提升FL在动态环境中的效率。
    * 关键分析：分别详细分析了本地更新和上传更新两个阶段的能量消耗，为绿色优化奠定基础。
* DDAFL智能调度算法（核心创新）：
    * 目标：最小化总能量消耗。
    * 机制：利用深度强化学习，在每个训练轮次中，根据MEC网络的实时状态（如设备可用性、数据分布、信道条件），智能决策参与全局聚合的移动设备数量。
    * 权衡的艺术：这实现了对 “收敛稳定性”（需要一定数量的设备参与以保证更新质量）和 “通信能耗”（减少不必要的设备上传）的动态最优平衡。

# Advances and open problems in federated learning

文章开宗明义，给出了联邦学习的经典定义：在中心服务器协调下，多个客户端协作训练模型，同时保持训练数据去中心化。
术语起源：该术语由McMahan等人在2016年引入，其核心设定包含了数据非独立同分布、客户端海量且不可靠、通信带宽受限这一系列相互交织的挑战。
广义定义：文章进一步提出了一个更广泛的定义，强调 “聚焦更新”（传输最小必要信息）和 “尽早聚合”（服务于数据最小化原则），并以此区别于完全去中心化的对等学习。

论文创造性地将联邦学习划分为两大代表性场景，这一分类深刻影响了后续研究：
跨设备联邦学习：原始设定，涉及海量移动或边缘设备（如手机）。特点是客户端数量极多、不可靠、数据异构性强、资源严格受限。
跨孤岛联邦学习：涉及少量相对可靠的组织或数据孤岛（如医院、银行）。特点是客户端数量少、相对可靠、数据规模大但异构、隐私和安全要求极高。
文章指出，后续讨论将主要围绕更具挑战性的“跨设备”场景展开。

实际应用：列举了谷歌（Gboard）、苹果（Siri）等公司的实际产品应用，证明联邦学习已走出实验室。
生态工具：提到了TensorFlow Federated等开源框架，表明技术生态正在形成。
核心洞见：明确指出联邦学习的根本问题是高度跨学科的。解决它需要融合机器学习、分布式优化、密码学、安全、差分隐私、公平性、系统、信息论、统计学等多个领域的知识。最难的问题正处在这些领域的交叉点上。

# Federated machine learning: Concept and applications

文章开篇精准地指出了制约当今AI发展的两个结构性矛盾：

* 数据孤岛：在绝大多数行业，数据以“孤岛”形式存在。由于行业竞争、隐私安全和复杂的行政程序，跨组织甚至跨部门的数据整合都阻力巨大、成本高昂。
* 日益强化的数据隐私与安全法规：以欧盟GDPR为代表的全球性数据保护法规的出台，从根本上限制了传统的数据收集、融合与交易模式。过去那种简单地将数据汇集到一处的做法，现在可能面临法律风险。

提出解决方案：全面的安全联邦学习框架

面对“数据既割裂又不能随意集中”的困境，文章旗帜鲜明地提出了安全联邦学习作为根本出路。

* 超越谷歌的原始框架：在谷歌2016年提出的联邦学习概念基础上，本文提出了一个更全面的“安全联邦学习”框架。
* 系统性分类：框架包含三大类，覆盖了数据在不同维度上对齐的场景：
    * 横向联邦学习：适用于各参与方数据特征重叠多、但样本重叠少的场景（如不同地区的银行）。
    * 纵向联邦学习：适用于各参与方样本重叠多、但特征重叠少的场景（如银行与电商关于同一批用户）。
    * 联邦迁移学习：适用于样本和特征重叠都很少的场景，利用迁移学习技术实现知识共享。
* 愿景升华：提出基于联邦机制在组织间构建数据网络，作为在不损害用户隐私的前提下实现知识共享的有效解决方案。

核心主张：AI发展重心的战略转移

文章提出了一个具有深远影响的呼吁：AI发展的重点应从当前大多数研究所关注的“提升模型性能”，转向“研究与数据隐私安全法规相兼容的数据整合方法”。这标志着研究范式从技术驱动转向 “合规与价值驱动”。

# Distributed training strategies for the structured perceptron

提出了结构化感知机适用的分布式训练方法。

论文指出，结构化感知机在NLP任务中非常流行，但面临训练瓶颈：

* 数据增长：训练集规模不断增大。
* 计算复杂度高：训练复杂度与推理过程成正比，而后者通常是序列长度的非线性函数（即使有强结构独立性假设），导致训练缓慢。

为了解决分布式训练问题，论文探索并比较了两种策略：

* 简单参数混合：
    * 做法：将训练数据划分为不相交的子集，在不同机器上并行训练独立的模型，最后将所有模型的参数进行平均，作为最终模型。
    * 问题：研究发现，感知机不适合这种简单的参数混合，即使它被广泛用于大规模结构化学习。
* 迭代参数混合：
    * 做法：对简单参数混合进行细微修改，形成迭代过程。具体迭代机制未在此段详述，但很可能是进行多轮“并行训练-参数平均-再分发”的循环。
    * 优势：论文证明迭代参数混合具有以下优良性质：

具有与标准感知机算法相似的收敛性。 如果训练集是线性可分的，它能找到一个分离超平面。 能显著减少训练时间。 产生的模型准确率与串行训练所有数据得到的模型相当甚至更优。

# Deep learning with Elastic Averaging SGD

提出了一种新的分布式优化算法Elastic Averaging SGD (EASGD)，该算法使本地工作者能够进行更多探索，即通过减少本地工作者与主节点之间的通信量，允许局部变量更远地偏离中心变量。

论文指出，在深度学习时代，一个核心挑战是设计能在多个GPU卡上实现显著加速的并行SGD算法，以训练大规模模型（如卷积神经网络）。当时，成功的图像识别系统通常在单机多卡上运行。

EASGD 的核心思想是对经典的平均SGD算法进行并行化扩展，其设计受二次惩罚法启发，但进行了重新诠释。

* 基本架构：采用主从式架构。每个工作节点维护自己的本地参数；一个主节点维护一个中心变量。
* 关键创新——弹性力：工作节点与主节点之间通过一个弹性力进行协调。该力将每个工作节点的本地参数向中心变量“拉拽”，同时中心变量也受到所有工作节点参数的影响。
* 更新方式：中心变量的更新是在时间和空间上的移动平均（既随时间平滑，又是对所有工作节点参数的空间平均）。这允许工作节点有一定程度的局部探索自由，但又被弹性力约束在全局中心附近。

主要优势与贡献：

* 快速收敛：相比当时主流的异步方法，EASGD提供了更快的收敛速度。
* 通信高效：减少了主节点与工作节点之间的通信开销。
* 性能优越：在测试误差上保持了高质量的性能，优于DOWNPOUR等基准方法。
* 适用性强：适用于卷积神经网络等深度学习模型的并行化训练。

# Information-theoretic lower bounds for distributed statistical estimation with communication constraints

系统研究了在分布式统计估计中，通信受限条件下的性能极限。他们采用信息论分析方法，推导出在多个节点间数据分散且通信带宽受限的情况下，各类统计估计任务（如参数估计、回归分析等）所需的最小通信量下界。

论文聚焦于一个根本性的理论问题：在分布式统计估计中，通信成本与统计精度之间的根本性权衡是什么？

* 场景：将总数为N的样本均匀分配给m台机器（每台n=N/m个样本）。每台机器可以对自己的数据进行任意处理，然后通过通信（向中央节点或彼此）发送中间结果。
* 核心问题：为了达到集中式方案（能够访问所有N个样本）所能达到的最优估计误差，分布式方案必须交换的最小比特数（通信预算B） 是多少？
* 研究方法：采用统计极小极大理论来形式化这个问题，即在给定的通信预算B下，基于所传递信息的所有估计器的极小极大风险下界。

理论贡献与目标：

* 建立下界：论文旨在为分布式统计估计提供极小极大下界。通过将下界与集中式估计的最优性能进行比较，可以识别出分布式估计器为了获得与集中式估计器相当的性能，所必须支付的最小通信成本。
* 刻画极限：研究当机器数量m和数据维度变化时，性能如何变化。这有助于理解分布式估计的可扩展性。
* 达到极限：论文表明，可以利用近期的工作来实现这些理论极限，即构造出通信高效的、能达到或接近该下界的分布式估计算法。

# A comprehensive survey on communication-efficient federated learning in mobile edge environments

针对这一挑战，本综述全面总结了提升联邦学习通信效率的方法，重点关注：

1）最小化通信复杂度以降低总传输量；
2）合理调度资源以提高训练效率；
3）利用空中计算（OTA）将计算融入通信，以适应移动边缘环境中联邦学习的计算/通信特性。

因此，本研究从融合与数据异构性视角出发，通过优化算法性能减少通信轮次。我们期望本综述能为未来研究提供关于通信高效联邦学习的启示。

# Dynamic federated learning-based economic framework for internet-of-vehicles

尽管如此，在物联网网络中实施联邦学习仍面临重大挑战，例如来自大量服务提供商的动态活动和多样化的信息质量（QoI）、服务提供商的有限支付预算，以及服务提供商之间的利润竞争。本文提出一种基于联邦学习的动态经济框架，旨在解决物联网网络中的这些挑战。具体而言，服务提供商首先实施车辆选择机制，根据每轮学习中车辆当前位置的重要性及其信息历史，确定最优车辆集参与联邦学习。随后，每台选定车辆可采集道路信息，并根据所收集的QoI向VSP提交支付合约。为此，我们开发了多主体单代理合约策略，在VSP有限支付预算及双方信息不对称的条件下，实现VSP与学习车辆的利润最大化。通过基于真实道路数据集的实验验证，我们的框架在网络中仅10%主动SV参与时，收敛速度仍可提升57%，且能实现高达27.2倍的网络社会福利增益。

论文精准地指出了在动态的车联网环境中应用传统联邦学习范式的三大难题：

* 车辆动态性与通信开销：车辆频繁移动、连接不稳定，且数量庞大，对所有车辆进行训练和模型收集成本高昂、不切实际。
* 信息质量异构性：不同车辆在不同时间和地点收集的信息质量差异巨大。低质量数据会损害模型精度和训练稳定性。
* 激励机制与信息不对称：在服务商预算有限、车辆间存在利益竞争、且服务商与车辆间存在信息不对称（如服务商预算不公开）的情况下，如何设计激励措施来吸引“合适”的车辆贡献高质量数据，并最大化各方收益，是一个开放性难题。

论文提出了一个名为 “动态联邦学习经济框架” 的完整解决方案，包含以下核心环节：

* 动态车辆选择策略：
    * 双重重要性筛选：首先基于位置重要性（利用平均日交通流量数据）筛选活跃车辆；再基于信息重要性历史筛选出“最佳”车辆。
    * 目的：确保入选车辆既能提供地理上关键区域的数据，又具备提供高质量、可靠模型更新的潜力。
* 基于合约的经济激励机制：
    * 模型创新：采用多委托方-单代理方合约模型。最佳车辆作为“委托方”非合作地提供包含其信息质量和要价的合约；车联网服务商作为“代理方”负责优化并选择合约。
    * 解决核心矛盾：此模型天然适用于信息不对称（服务商预算保密）和车辆间存在竞争的场景。通过满足个人理性和激励相容约束，确保服务商愿意参与且选择对其最有利的合约。
* 联邦学习执行与收益更新：
    * 基于选定的车辆执行FL算法。
    * 每轮结束后，服务商根据本轮全局模型精度和模型新鲜度来更新其净收益，将学习效果直接与经济回报挂钩。

主要贡献与效果：

* 提出新颖框架：首个为车联网设计的、融合动态选择与合约激励的FL经济框架。
* 设计有效选择方法：综合考虑位置与QoI，提升学习质量和收益。
* 开发MPOA合约问题与高效求解：建模车辆竞争与预算未知问题，并提供轻量级迭代求解算法。
* 理论分析与实验验证：分析了FL收敛性，并利用英国真实道路数据集验证了框架有效性，相比基线方法，能将网络社会福利提升高达27.2倍，学习收敛速度加快57%。

# Fedrts: Federated robust pruning via combinatorial thompson sampling

为应对这些挑战，我们提出基于组合汤普森采样的联邦鲁棒剪枝（FedRTS）——一种旨在构建鲁棒稀疏模型的新型框架。FedRTS通过基于汤普森采样的调整机制（TSAdj）提升鲁棒性与性能，该机制采用基于稳定且具有远见信息的概率决策，而非传统方法依赖的不稳定且短视的确定性决策。

论文首先指出，尽管现有的联邦剪枝框架通过“内环更新权重、外环调整拓扑”的两阶段训练避免了稠密模型训练，但其在外环的模型拓扑调整上存在三个关键挑战：

* 贪婪的短视调整：仅基于一小部分参与客户端的聚合信息进行决策，无视了大多数未参与客户端的数据和先验知识，导致调整短视且鲁棒性差。
* 不稳定的拓扑结构：基于异构数据分布的聚合信息所做的确定性调整，容易导致模型拓扑不稳定。
* 通信效率低下：为拓扑更新而传输大量辅助数据（如全尺寸梯度），通信成本高昂。

论文的核心洞察是将联邦剪枝重新建模为一个组合多臂老虎机问题，并将上述问题归因于短视观察和确定性决策。基于此，提出了 FedRTS 框架。

* 核心机制：基于汤普森采样的调整（TSAdj）
* 长远概率分布：利用包含未参与客户端先验信息的概率分布进行决策，以缓解部分客户端参与带来的偏差。
* 概率化决策：基于稳定、全面的信息进行概率性的拓扑调整决策，而非确定性的，从而增强稳定性。
* 通信高效：客户端仅需上传关键梯度的索引，而非完整的稠密梯度，大幅降低通信开销。

效果：在CV和NLP数据集上的实验表明，FedRTS能更好地处理客户端可用性和数据异构性，在准确率、泛化能力和通信成本上均优于现有方法。例如，在CIFAR-10/ResNet18上，能实现5.1%的准确率提升或33.3%的通信成本降低。

# Computing-aware network (CAN): a systematic design of computing and network convergence

论文指出，计算与网络的融合在数据中心内已有成功案例（如谷歌的全栈创新），但在广域网层面研究严重不足。现有的“天空计算”等多云方案仅停留在API抽象层，未触及底层网络，无法满足广域网特有的服务需求。存在三大核心问题：

* 计算服务调度不敏捷：跨多个边缘节点调度时，最近的节点未必是最佳处理节点。
* 数据分发模式不灵活：点对点跨节点数据传输导致带宽占用高、数据移动和复制开销大，不适合一对多等通信模式。
* 远距离损耗网络吞吐量低下：远距离大数据传输效率低，仍依赖高成本、不安全的物理硬盘运输。

为达成计算与网络在广域网层面的深度融合，论文提出了CAN 架构及其支撑的三大关键技术：

CAN 架构核心：在传统的控制平面和数据平面之外，新增一个“感知平面”。该平面负责收集、管理和融合计算与网络的多维信息，从而与增强的控制平面、数据平面形成一个闭环控制系统，实现联合优化。

三大关键技术：

* CATS：解决跨边缘节点调度时，计算信息高效、低开销传播及快速准确决策的挑战。
* 弹性广播：实现广域网中灵活、低成本的一对多通信，解决现有方案在转发状态维护或树状管理上的高开销问题。
* 广域高吞吐传输：针对远距离高丢包网络，突破传统TCP/UDP的吞吐限制，保障大数据传输效率。

# Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP

提出分层跳跃式联合学习方案：仅对预训练LLM的特定层进行跨客户端微调，其余层保持冻结状态。应用于LLaMA 3.2-1B模型时，该方案在保持性能与集中式训练偏差≤2%的前提下，将通信成本降低约70%

论文精准地切中了在医疗健康领域应用联邦学习与大语言模型的核心矛盾：医疗数据特性：包含敏感隐私信息，受法规严格保护，天然形成“数据孤岛”。

联邦学习引入LLM的四大障碍：

* 通信开销：LLM参数量巨大（十亿级），传输完整模型更新带宽成本极高。
* 数据异构性：不同医院的病历在患者群体、专科术语、记录习惯上差异巨大，导致严重的非独立同分布数据。
* 隐私漏洞：模型更新仍可能泄露信息，需要额外保护（如差分隐私）。
* 资源约束：各医疗机构算力不均，难以统一部署大模型。

论文提出了一个直观却高效的核心思想：冻结大部分，只微调关键部分。

* 具体做法：在联邦微调一个预训练好的大语言模型（如LLaMA 3.2-1B）时，仅选择其中一部分层进行更新和通信，其余大部分层保持冻结。
* 内在逻辑：预训练模型已经学习了强大的通用语言表示。对于下游的医疗专业任务，可能只需要调整模型高层的、与特定任务更相关的表示即可，无需动底层通用特征。

显著效果：

* 通信成本：降低约 70%。
* 模型性能：在临床NER和分类任务上（使用i2b2和MIMIC-III数据集），性能保持在集中式训练的 98-99% 以内（即损失不超过2%）。
* 附加优势：提升了在非独立同分布临床数据下的收敛速度和鲁棒性，并且在与差分隐私结合时表现出更好的隐私-效用权衡。

# Federated learning with flexible architectures

提出具有灵活架构的联邦学习（FedFA），这是一种允许客户端训练不同宽度和深度模型的联邦学习训练算法。每个客户端可根据自身资源选择适配的网络架构，较浅较窄的网络在训练时所需计算资源更少。与该领域现有研究不同，FedFA在模型聚合过程中引入层嫁接技术，使客户端的本地架构与联邦学习系统中最大的网络架构保持一致。分层嫁接确保所有客户端贡献均匀融入全局模型，从而最大限度降低单个客户端数据导致模型参数失衡的风险，并带来安全效益。此外，FedFA引入可扩展聚合方法以管理不同网络架构间权重比例的差异。

研究背景与问题：

* 联邦学习（FL）的需求：在医疗影像、军事等敏感领域，需要在保护数据隐私的前提下进行联合训练。
* 传统FL的局限性：难以有效整合计算和通信资源各异的客户端（如战机与坦克、大医院与小诊所），通常只能让弱客户端减少计算量，但会拖慢收敛速度。
* 异构架构FL的兴起：允许客户端根据自身资源定制模型深度或宽度（如HeteroFL、FlexiFed等方法），提升效率。

现有方法的缺陷：

* 安全问题：
    * 不同架构的客户端模型在聚合时会产生“弱点”（weak points），即某些权重仅由少数客户端更新。
    * 攻击者可利用这些弱点进行后门攻击，篡改模型行为。
* 公平性问题：
    * 不同架构的权重存在尺度差异（scale variations），导致全局模型偏向某些客户端的数据，造成聚合不公平。

FedFA的解决方案：
* 统一聚合策略：
    * 构建一个深度和宽度最大的全局模型，确保每个客户端都能参与所有权重的更新，消除“弱点”。
    * 利用神经网络中跳跃连接（skip connections）的相似性，对齐不同客户端的层进行聚合。
* 公平尺度调整：
    * 提出公平可伸缩的聚合方法，消除权重尺度差异带来的偏差。
* 架构优化：
    * 结合神经架构搜索（NAS），为每个客户端根据其数据特性设计最优模型架构。

主要贡献：

* 首次系统解决异构FL的安全问题，通过均匀聚合层参数防御后门攻击。
* 首次有效处理动态训练中的尺度变异问题，实现公平聚合。
* 引入NAS优化客户端架构，提升本地与全局性能。

实验验证：

* 在Pre-ResNet、MobileNetV2、EfficientNetV2等模型上，FedFA在IID和非IID数据设置中均优于现有方法（全局精度提升最高达1.20倍）。
* 显著提升对抗后门攻击的鲁棒性（相比基线，精度下降减少最多达3.31倍）。
* 在Transformer语言模型上，困惑度（perplexity）改善达1.07–4.50倍。

# Incentive-aware autonomous client participation in federated learning

研究背景与问题：

* 联邦学习的现实瓶颈：
    * 传统FL假设参数服务器（PS）能完全决定客户端的参与，但实际中客户端参与训练会产生计算、能耗等成本，因此客户端需要激励才愿意参与。
    * 现有激励机制大多假设客户端完全理性且信息完备，但现实中客户端往往是非合作、信息不完全的（只知道自身效用）。
* 核心矛盾：
    * PS每轮提供固定预算奖励，由参与客户端共享。
    * 参与客户端越多，每个客户端分得的奖励越少，可能导致奖励无法覆盖成本；反之，参与过少又会影响训练效率。

解决方案：少数派游戏（MG）建模

* 将客户端自主参与建模为少数派游戏：
    * 客户端独立决定是否参与每轮训练，以最大化自身效用（奖励减成本）。
    * 当参与客户端数低于“受益客户端数上限”时，参与者处于少数派并获胜（即获得正效用）。
* 挑战：
    * 客户端缺乏协调导致每轮参与数量波动大，影响模型收敛与精度。

提出的三种MG算法

* 标准MG方案：
    * 客户端基于历史信息决策，但参与波动性很高。
* 随机MG方案：
    * 改进：上一轮获胜的客户端保持决策不变，其余客户端以一定概率改变决策。
    * 理论证明：可显著降低参与波动性。
* 联盟MG方案：
    * 进一步改进：客户端形成多个联盟，联盟内客户端协调做出相反决策，以最大化长期获胜概率。
    * 效果：进一步降低波动性，提升整体效用。

# Heterogenous federated learning via model distillation

联邦学习的传统假设存在局限：

* 传统FL要求所有客户端使用相同的模型架构来训练一个中心模型。
* 现实中，在许多商业场景（如医疗、金融、AI服务）中，参与者有能力和需求设计自己的专属模型。这些模型是：
* 为不同任务定制的（例如，不同公司的客服聊天机器人功能不同）。
* 涉及知识产权和商业机密，不能公开架构细节。

提出的核心问题： 如何在一个联邦框架中，让每个参与者使用完全不同的、对彼此是“黑盒”的模型进行协作学习，同时不共享数据和模型架构？

解决方案：FedMD框架

* 核心思想：
    * 利用迁移学习和知识蒸馏来构建一个通用的通信协议，使不同架构的模型能够相互“理解”和传递知识。
* 关键设计：
    * 公共数据集作为“交流媒介”：所有模型在一个公共数据集（如ImageNet）上进行预训练和后续的知识交换。
    * 基于知识蒸馏的通信：
        * 每个模型在本地用自己的私有数据训练。
        * 每个模型在公共数据集上计算并上传其输出类别概率分布（软标签）给中央服务器。
        * 服务器聚合这些软标签（例如，取平均），生成一个“共识知识”。
        * 服务器将共识知识下发给所有模型，各模型以此为目标，在公共数据集上进行知识蒸馏，从而将从其他模型学到的知识融入自身。
    * 黑盒操作：服务器和参与者都无需知道其他模型的内部架构，只需能获取其输入-输出（即推理结果）。

主要贡献与效果

提出了FedMD框架：

* 这是首个系统性地解决完全模型异构（而不仅仅是数据异构）的联邦学习框架。
* 实现了在保护数据隐私和模型知识产权双重前提下的协作学习。

实验验证：

* 在MNIST/FEMNIST和CIFAR-10/CIFAR-100数据集上进行测试。
* 效果显著：在10个不同参与者的设置下，每个本地模型的最终测试准确率平均提升了20%（相比于不协作的孤立训练）。
* 接近理想上限：其性能仅比将所有私有数据集中在一起训练的“理想上限”低几个百分点。

# {eSGD}: Communication efficient distributed deep learning on the edge

边缘计算的趋势与潜力：

* 现代物联网设备（如手机、传感器、摄像头）计算能力不断增强，且能收集丰富的实时本地数据，非常适合进行本地机器学习训练。
* 本地训练能提供更好的隐私保护、个性化服务，并避免传统“云训练”模式的高延迟、低吞吐量和昂贵数据成本。

核心挑战：通信瓶颈： 在边缘分布式训练框架中（边缘设备作为计算节点，边缘服务器作为参数服务器），频繁同步梯度和模型参数会产生巨大的网络通信开销，成为主要的性能瓶颈。

解决方案：eSGD（边缘随机梯度下降）

eSGD是一系列梯度稀疏化方案，通过两个核心机制在保证收敛性的同时，大幅降低通信成本。

重要性更新：

* 核心思想：并非所有梯度坐标都同等重要。eSGD通过一种历史偏好（隐藏权重） 机制，识别并只选择重要的梯度坐标传输给服务器进行同步。
* 实现：为每个梯度坐标维护一个“隐藏权重”。每次坐标参与同步，其权重增加。权重大的坐标被认为是“重要的”，在后续轮次中被选中的概率更高。这样可以激进地减少需要传输的数据量（例如丢弃50%-87.5%的梯度）。

动量残差累积：

* 解决的问题：简单地丢弃大量梯度会导致信息丢失，降低模型收敛速度和最终精度。
* 解决方案：为那些未被选中传输的梯度坐标维护一个“残差累积项”。这些未被同步的梯度值不会直接丢弃，而是被累积起来，并在未来的更新时机（当该坐标被选为“重要”时）加入更新。这相当于跟踪并延迟应用了这些“过时”的梯度，确保了训练信息的完整性，避免了收敛率下降。

主要贡献与效果：

* 提出了eSGD方法：
    * 为边缘分布式训练场景量身定制，在理论（保证收敛）和实用（保证性能）层面都提供了保障。
    * 巧妙结合了梯度稀疏化（降低通信） 和残差补偿（保证收敛） 两大技术。
* 实验验证：
    * 在经典的MNIST手写数字数据集上进行了验证。
    * 效果显著：即使在丢弃87.5%梯度坐标的极端稀疏情况下，模型仍能达到81.5%的准确率。当丢弃比例为75%和50%时，准确率分别达到86.7% 和91.2%，证明了eSGD在大幅降低通信成本的同时，能有效维持模型性能。

# Privacy-Preserving Federated Learning with Homomorphic Encryption and Sparse Compression

研究背景与问题

* 联邦学习的隐私威胁：
    * 尽管FL旨在保护原始数据，但其共享的模型更新仍面临成员推理攻击、模型反演攻击等隐私威胁。
* 主流隐私保护技术及其缺陷：
    * 差分隐私：通过添加噪声保护隐私，但通常会显著降低模型精度。
    * 同态加密：允许在密文上直接进行计算，能有效保护隐私且对模型精度影响极小，但会带来巨大的计算负担和通信开销（密文体积远大于明文），难以在资源受限的客户端部署。

为了在保持同态加密强隐私保护优势的同时，克服其性能瓶颈，本文提出一种联合优化方法：

* 模型参数稀疏化：
    * 在上传前，大幅减少本地模型更新中需要加密的参数数量，仅保留最重要的部分。这直接降低了需要处理的加密数据量。
* 压缩感知：
    * 对稀疏化后的参数进一步应用压缩感知技术，用远低于传统需求的测量值（数据量）来表征模型更新，极大地压缩了需要传输的数据包大小。
* 集成工作流：
    * 客户端：本地更新 -> 稀疏化 -> 压缩感知 -> 同态加密 -> 上传密文压缩数据。
    * 服务器：直接对收到的密文压缩数据执行聚合操作（如FedAvg），得益于同态加密和线性压缩的特性，无需先解密。这同时减少了上行和下行的通信开销。

主要贡献与效果；

* 提出了一种高效的隐私保护FL方案：
* 首次系统地将稀疏化、压缩感知与同态加密结合，以应对HE的固有开销。
* 实现了隐私性、模型精度和系统效率三者间更优的平衡。
* 实验验证（基于Paillier同态加密）：
    * 计算开销大幅降低：
        * 加密时间：从 7.75秒降至0.70秒（降低 90%）。
        * 解密时间：从 5.23秒降至0.66秒（降低 88%）。
    * 通信开销大幅降低：
        * 传输数据量：从 2494.56 KB降至163.78 KB（降低 93.4%）。
    * 模型精度几乎无损甚至提升：
    * MNIST：从98.47%微降至98.37%。
    * Fashion-MNIST：从89.64%提升至90.16%。这表明去除冗余参数可能起到了正则化效果。
